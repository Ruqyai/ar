---
title: 'Fine-Tune Gemma 2 2b باستخدام Keras و LoRA (الجزء الثالث)'
date: 2024-09-12
permalink: /posts/2024/09/blog-post-5/
tags:
  - Gemma2
  - LLM
  - Generative AI

---
![Gemma2](https://raw.githubusercontent.com/Ruqyai/ruqyai.github.io/main/images/gemma2.png)
استكمالًا لسلسلة الدروس التعليمية التي تركز على اللغة العربية في التعامل مع النماذج اللغوية الضخمة، في هذا الجزء سنواصل استكشاف كيفية إعادة ضبط نموذج Gemma2-9b على مجموعة بيانات عربية باستخدام مكتبة Keras وKeras_nlp وتقنية LoRA. سنتناول كيفية إعداد البيئة، تحميل النموذج، إجراء التعديلات اللازمة، ثم تدريب النموذج باستخدام التوازي النموذجي لتوزيع معلمات النموذج عبر عدة وحدات تسريع.

### Fine-Tune Gemma2 9b باستخدام Keras وLoRA (الجزء الثالث)

### نظرة عامة

نموذج Gemma هو مجموعة من النماذج الخفيفة والمطورة التي تم إنشاؤها باستخدام أبحاث وتقنيات مشابهة لتلك المستخدمة في نماذج Google Gemini. يمكن تعديل نماذج Gemma لتلبية احتياجات محددة، ولكن قد يكون من الصعب التعامل مع النماذج الكبيرة جدًا على وحدة تسريع واحدة.

في حالة نموذج Gemma2-9b، قد يكون حجمه كبيرًا جدًا ليتناسب مع وحدة تسريع واحدة مثل Kaggle، مما يجعل عملية التعديل والتنبؤ صعبة. هناك بعض الخيارات المتاحة مثل التكميم لتسهيل الاستدلال، أو استخدام التوازي النموذجي لتوزيع معلمات النموذج عبر عدة وحدات تسريع.

### خطوات إعداد البيئة

#### 1. **قبول شروط الاستخدام:**
   - تأكد من قبولك لشروط استخدام Gemma من خلال صفحة [Keras Gemma 2](https://keras.io/gemma2/).

#### 2. **تثبيت المكتبات:**
   ```bash
   !pip install -q -U keras-nlp tensorflow-text
   !pip install -q -U tensorflow-cpu
   !pip install -q -U huggingface_hub
   !pip install -q -U datasets
   ```

#### 3. **إعداد Keras مع JAX:**
   ```python
   import jax
   jax.devices()

   import os
   os.environ["KERAS_BACKEND"] = "jax"
   os.environ["XLA_PYTHON_CLIENT_MEM_FRACTION"] = "1.0"
   ```

### تحميل النموذج وتكوين التوزيع

#### 4. **تحميل النموذج:**
   ```python
   import keras
   import keras_nlp

   device_mesh = keras.distribution.DeviceMesh(
       (1, 8),
       ["batch", "model"],
       devices=keras.distribution.list_devices(),
   )
   model_dim = "model"

   layout_map = keras.distribution.LayoutMap(device_mesh)

   layout_map["token_embedding/embeddings"] = (model_dim, None)
   layout_map["decoder_block.*attention.*(query|key|value)/kernel"] = (model_dim, None, None)
   layout_map["decoder_block.*attention_output/kernel"] = (model_dim, None, None)
   layout_map["decoder_block.*ffw_gating.*/kernel"] = (None, model_dim)
   layout_map["decoder_block.*ffw_linear/kernel"] = (model_dim, None)
   model_parallel = keras.distribution.ModelParallel(
       layout_map=layout_map,
       batch_dim_name="batch",
   )

   keras.distribution.set_distribution(model_parallel)
   gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset("gemma2_9b_en")
   ```

#### 5. **إجراء اختبار للتنبؤ قبل التعديل:**
   ```python
   print(gemma_lm.generate("أنشئ خمسة أسماء مستخدم واقعية بالعربي.", max_length=512))
   ```

### إعداد مجموعة البيانات

#### 6. **تحميل مجموعة بيانات CIDAR:**
   ```python
   from datasets import load_dataset

   dataset = load_dataset("arbml/CIDAR", split="train")
   df = dataset.to_pandas()
   df.head(10)
   ```

#### 7. **تنسيق البيانات:**
   ```python
   import json

   data = []

   for index, row in df.iterrows():
       template = "Instruction:\n{instruction}\n\nResponse:\n{output}"
       formatted_example = template.format(instruction=row['instruction'], output=row['output'])
       data.append(formatted_example)

   data = data[:2500]
   ```

### تفعيل LoRA وتدريب النموذج

#### 8. **تفعيل LoRA للنموذج:**
   ```python
   gemma_lm.backbone.enable_lora(rank=8)
   gemma_lm.preprocessor.sequence_length = 512
   gemma_lm.compile(
       loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),
       optimizer=keras.optimizers.Adam(learning_rate=5e-5),
       weighted_metrics=[keras.metrics.SparseCategoricalAccuracy()],
   )
   gemma_lm.summary()
   ```

#### 9. **بدء التدريب:**
   ```python
   gemma_lm.fit(data, epochs=1, batch_size=4)
   ```

#### 10. **إجراء اختبار للتنبؤ بعد التعديل:**
    ```python
    print(gemma_lm.generate("Instruction:\n أنشئ خمسة أسماء مستخدم واقعية بالعربي.\n\nResponse:\n", max_length=512))
    ```

### حفظ النموذج المعدل

#### 11. **إعداد الدخول وحفظ النموذج على Hugging Face:**
    ```python
    from kaggle_secrets import UserSecretsClient
    from huggingface_hub import login

    user_secrets = UserSecretsClient()
    HF = user_secrets.get_secret("HF_WRITE")
    login(token=HF)

    gemma_lm.save("hf://Ruqiya/gemma2_9b_en_fine_tuned_arabic_dataset")
    ```

### الخاتمة

بهذا نكون قد أكملنا الجزء الثالث من سلسلة دروسنا حول ضبط نموذج Gemma2-9b باستخدام Keras وLoRA. من خلال هذه الخطوات، تعلمت كيفية تحسين أداء النماذج الكبيرة للبيانات باللغة العربية وتوفير نموذج مخصص يمكنه تنفيذ مهام متعددة. كما تعلمت كيفية رفع النموذج على منصة Hugging Face ليكون متاحًا للتجربة والاستخدام في المشاريع المستقبلية.

**يرجى ملاحظة** أن هذا الدرس التعليمي يهدف إلى فهم الخطوات الأساسية، ولكن لتحقيق أداء مرضٍ، قد تحتاج إلى خطوات تحسين إضافية.

لمزيد من الاستكشاف، يمكنك الاطلاع على الروابط أدناه.


## روابط
- [Youtube and Slides](https://ruqyai.github.io/talks/2024-09-12-talk)
- [Notebook on Kaggle](https://www.kaggle.com/code/ruqiyas/arabic-finetune-gemma2-9b-model-using-keras-lora)
- [Finetuned Model on Huggingface(Kaggle)](https://huggingface.co/Ruqiya/gemma2_9b_en_fine_tuned_arabic_dataset)  

- [Read more on my Medium](https://medium.com/@rbinsafi/fine-tune-gemma-2-2b-using-transformers-and-qlora-part-2-946b642951af)

