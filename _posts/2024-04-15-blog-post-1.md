---
title: '**توليد المعلومات المدعوم بالاسترجاع (RAG) باستخدام Gemma لشرح مفاهيم علم البيانات الأساسية**'
date: 2024-04-15
permalink: /posts/2024/04/blog-post-1/
tags:
  - Gemma
  - RAG
  - LLM
  - Generative AI
  - Gemma-1.1-2b-it
---

عالم علم البيانات يمكن أن يكون مرعبًا للمبتدئين، حيث يمتلئ بالمصطلحات المعقدة والمفاهيم المعقدة. ولكن ماذا لو كان لديك مساعد ذكاء اصطناعي بجانبك، جاهز لشرح هذه المفاهيم بعبارات بسيطة وإرشادك خلال عملية التعلم؟ هذا هو المكان الذي يأتي فيه قوة **توليد المعلومات المدعوم بالاسترجاع (RAG)**.

---

**لماذا RAG وGemma؟**

RAG هي تقنية قوية تجمع بين نموذج استرجاع (لإيجاد المعلومات ذات الصلة) ونموذج توليد (لإنشاء نص يشبه النص البشري). هذا يجعلها مثالية لمهام مثل شرح المواضيع المعقدة، حيث يمكنها الوصول إلى كميات هائلة من المعلومات ومعالجتها ومن ثم تقديمها بطريقة واضحة ومفهومة.

لقد اخترنا نموذج اللغة **Gemma-1.1-2b-it**، الذي طورته Google AI، كنموذج التوليد لدينا. يعد Gemma معروفًا بدقته وكفاءته وسهولة استخدامه، مما يجعله رفيقًا مثاليًا لنظام RAG الخاص بنا.

---

# توليد المعلومات المدعوم بالاسترجاع (RAG) باستخدام Gemma لشرح مفاهيم علم البيانات الأساسية

في هذا الدليل، سنستخدم نموذج توليد المعلومات المدعوم بالاسترجاع (RAG) لشرح مفاهيم علم البيانات الأساسية. RAG يجمع بين نموذج استرجاع ونموذج لغة لتقديم استجابات ذات صلة ودقيقة للأسئلة.

![Retrieval Augmented Generation (RAG)](https://i.ibb.co/VwHwkw7/Retrieval-Augmented-Generation-RAG.png)

---

# الخطوة 1: تثبيت الحزم المطلوبة

أولاً، لنقم بتثبيت الحزم الضرورية باستخدام pip:

```bash
pip install transformers accelerate bitsandbytes langchain sentence-transformers chromadb gradio huggingface_hub
```

### ملاحظة: يجب إعادة تشغيل دفتر الملاحظات الخاص بك لتجنب أخطاء حزمة Gradio

---

# الخطوة 2: استيراد المكتبات المطلوبة

بعد ذلك، لنقم باستيراد المكتبات التي سنحتاجها:

```python
import os
import gradio as gr
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import WebBaseLoader
from langchain_community.vectorstores import Chroma
from langchain.chains import ConversationalRetrievalChain
from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings
from langchain_community.llms import HuggingFaceEndpoint
```

---

# الخطوة 3: تحميل البيانات

لنقم بتحميل البيانات من مصدر عبر الويب. في هذا المثال، سنستخدم مسرد علم البيانات:

```python
loader = WebBaseLoader("https://www.datascienceglossary.org/")
data = loader.load()
```

---

# الخطوة 4: تقسيم المستندات

لتحسين الكفاءة، سنقوم بتقسيم المستندات إلى أجزاء أصغر:

```python
text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)
splits = text_splitter.split_documents(data)
```

---

# الخطوة 5: إنشاء قاعدة بيانات المتجهات

سنستخدم SentenceTransformer لتضمين النص وإنشاء قاعدة بيانات المتجهات:

```python
embedding = SentenceTransformerEmbeddings(model_name='all-MiniLM-L6-v2')
vectordb = Chroma.from_documents(documents=splits, embedding=embedding)
```

---

# الخطوة 6: إنشاء مسترجع

الآن، سنقوم بإنشاء مسترجع باستخدام قاعدة بيانات المتجهات:

```python
retriever = vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 2})
```

---

# الخطوة 7: تحميل نموذج اللغة

سنستخدم نموذج لغة مُدرّب مسبقًا من Hugging Face:

```python
os.environ["HUGGINGFACEHUB_API_TOKEN"] = 'API_TOKEN'

repo_id = "google/gemma-1.1-2b-it"

llm = HuggingFaceEndpoint(repo_id=repo_id, max_length=1024, temperature=0.1)
```

---

# الخطوة 8: إنشاء سلسلة مسترجع المحادثة

دمج المسترجع ونموذج اللغة في سلسلة مسترجع المحادثة:

```python
qa = ConversationalRetrievalChain.from_llm(llm, retriever)
```

---

# الخطوة 9: تعريف دالة تنفيذ المحادثة

تعريف دالة لتنفيذ المحادثة:

```python
def execute_conversation(question):
    chat_history = []
    result = qa({"question": question, "chat_history": chat_history})
    chat_history.append(result["answer"])
    return result["answer"]
```

---

# الخطوة 10: تعريف الأسئلة والحصول على الإجابات

الآن، يمكنك تعريف الأسئلة والحصول على الإجابات باستخدام دالة `execute_conversation`:

```python
question1="What is data science?"
print(execute_conversation(question1))
question2="What is correlation?"
print(execute_conversation(question2))
question3="What is Mean Squared Error?"
print(execute_conversation(question3))
```

---

# الخطوة 11: إنشاء واجهة Gradio

أخيرًا، إنشاء واجهة Gradio للتفاعل مع النموذج:

```python
chatbot = gr.Interface(
    fn=execute_conversation,
    inputs="text",
    outputs="text",
    live=False,
    title="RAG using Gemma to explain basic data science concepts.",
    description="Enter your question",
)

chatbot.launch()
```

---

## النتوبوك - الكود
[My notebook](https://www.kaggle.com/code/ruqiyas/rag-using-gemma-to-explain-data-science-concepts)

```
@misc{data-assistants-with-gemma,
    author = {Paul Mooney, Ashley Chow},
    title = {Google – AI Assistants for Data Tasks with Gemma},
    publisher = {Kaggle},
    year = {2024},
    url = {https://kaggle.com/competitions/data-assistants-with-gemma}
}
```