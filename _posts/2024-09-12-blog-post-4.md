---
title: 'Fine-Tune Gemma 2 2b باستخدام Transformers و qLoRA (الجزء الثاني)'
date: 2024-09-12
permalink: /posts/2024/09/blog-post-4/
tags:
  - Gemma2
  - LLM
  - Generative AI

---
![Gemma2](https://raw.githubusercontent.com/Ruqyai/ruqyai.github.io/main/images/gemma2.png)
### Fine-Tune Gemma 2 2b باستخدام Transformers و qLoRA (الجزء الثاني)

---

#### العنوان: Fine-Tune Gemma 2 2b باستخدام Transformers و qLoRA (الجزء الثاني)  
#### التاريخ: 12 سبتمبر 2024  
#### الرابط: /posts/2024/09/blog-post-4/  
#### الوسوم:  
- Gemma2  
- LLM  
- الذكاء الاصطناعي التوليدي  

---

![Gemma2](https://raw.githubusercontent.com/Ruqyai/ruqyai.github.io/main/images/gemma2.png)
استكمالًا لسلسلة الدروس التعليمية التي تركز على اللغة العربية في التعامل مع النماذج اللغوية الضخمة، سنواصل في الجزء الثاني استكشاف الأساليب المخصصة لإعادة ضبط نموذج `Gemma-2b` على مجموعة بيانات عربية لتحسين أدائه. سنستخدم مكتبة `Transformers` وتقنية `qLoRA` (التكيف منخفض الرتبة المكمم) لتقليل استهلاك الذاكرة.

ركز الجزء الأول على توليد النصوص المدعوم بالاسترجاع **[RAG](https://ruqyai.github.io/ar/posts/2024/07/blog-post-3/)**، بينما سنتناول في هذا الجزء ضبط `Gemma-2b` باستخدام مجموعة بيانات `arbml/CIDAR`، وهي مجموعة بيانات عربية تهدف لتحسين أداء النماذج في بعض المهام المتعلقة  باللغة العربية.

---

### Fine-Tune Gemma 2 2b باستخدام Transformers و qLoRA (الجزء الثاني)

سيأخذك هذا الدرس في جولة حول تحسين نموذج **Gemma-2b** باستخدام **qLoRA** على مجموعة بيانات عربية، **arbml/CIDAR**. تشمل العملية إعداد مجموعة البيانات، تكوين النموذج، تطبيق محولات **LoRA**، ودفع النموذج المحسن إلى HuggingFace.

---

#### حول مجموعة البيانات: CIDAR
تحتوي مجموعة البيانات **CIDAR** على **10,000 تعليمات** ومخرجاتها المقابلة. تم تنسيقها من خلال اختيار **9,109 عينات** من مجموعة بيانات **Alpagasus**، التي تمت ترجمتها إلى اللغة العربية باستخدام ChatGPT. تم الحصول على **891 تعليمات إضافية** حول قواعد اللغة العربية من موقع "اسأل المعلم". تمت مراجعة جميع الإدخالات من قبل 12 مراجعاً.

---

### الخطوة 1 - المتطلبات الأساسية

قبل البدء، تأكد من توفر التالي:

- **وحدة معالجة الرسوميات (GPU)**: يمكن تحسين نموذج Gemma-2b على **T4** GPU (متاحة مجانًا على Google Colab)، بينما يتطلب **Gemma-7b** استخدام **A100** GPU.

تحقق مما إذا كانت وحدة معالجة الرسوميات الخاصة بك مكتشفة:

```python
!nvidia-smi
```
- **حزم Python**: قم بتثبيت الحزم الضرورية باستخدام الأمر التالي:

```bash
!pip install transformers peft bitsandbytes datasets wandb
```

تسجيل الدخول إلى HuggingFace:

```python
from kaggle_secrets import UserSecretsClient
from huggingface_hub import login

user_secrets = UserSecretsClient()
HF_Read = user_secrets.get_secret("HF-Read")
login(token=HF_Read)
```

---

### الخطوة 2 - تحميل وتكوين النموذج

سنستخدم **BitsAndBytesConfig** لتكميم النموذج:

```python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

model_id = "google/gemma-2b-it"
model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={"": 0})
tokenizer = AutoTokenizer.from_pretrained(model_id, add_eos_token=True)
```

---

### الخطوة 3 - تنسيق مجموعة البيانات

لنقم بتحميل مجموعة البيانات **arbml/CIDAR** وتحويلها إلى تنسيق يمكن للنموذج فهمه:

```python
from datasets import load_dataset
dataset = load_dataset("arbml/CIDAR", split="train")

def generate_prompt(data_point):
    prefix_text = 'Below is an instruction that describes a task. Write a response that appropriately completes the request.\n\n'
    text = f"<start_of_turn>user {prefix_text} {data_point['instruction']} <end_of_turn>\n<start_of_turn>model {data_point['output']} <end_of_turn>"
    return text

# إضافة عمود 'prompt' إلى مجموعة البيانات
text_column = [generate_prompt(data_point) for data_point in dataset]
dataset = dataset.add_column("prompt", text_column)

# تقسيم مجموعة البيانات إلى تدريب واختبار
dataset = dataset.shuffle(seed=1234)
dataset = dataset.map(lambda samples: tokenizer(samples["prompt"]), batched=True)

dataset = dataset.train_test_split(test_size=0.2)
train_data = dataset["train"]
test_data = dataset["test"]
```

---

### الخطوة 4 - تطبيق LoRA

بعد ذلك، سنطبق **LoRA** (محولات منخفضة الرتبة) باستخدام مكتبة PEFT:

```python
from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model

model.gradient_checkpointing_enable()
model = prepare_model_for_kbit_training(model)

modules = find_all_linear_names(model)  # احصل على الوحدات لتطبيق LoRA عليها

lora_config = LoraConfig(
    r=64,
    lora_alpha=32,
    target_modules=modules,
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
```

---

### الخطوة 5 - التحسين

الآن، دعنا نقوم بتحسين النموذج باستخدام **SFTTrainer** من مكتبة **trl**:

```python
import transformers
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=test_data,
    dataset_text_field="prompt",
    peft_config=lora_config,
    args=transformers.TrainingArguments(
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,
        max_steps=10,
        learning_rate=2e-4,
        output_dir="outputs",
        optim="paged_adamw_8bit",
        save_strategy="epoch",
    ),
    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),
)

trainer.train()
```

---

### الخطوة 6 - حفظ ودمج النموذج

بعد الانتهاء من تحسين النموذج، سنقوم بدمج طبقات LoRA مع النموذج الأساسي وحفظه:

```python
base_model = AutoModelForCausalLM.from_pretrained(
    model_id,
    low_cpu_mem_usage=True,
    return_dict=True,
    torch_dtype=torch.float16,
    device_map={"": 0},
)
merged_model = PeftModel.from_pretrained(base_model, model)
merged_model = merged_model.merge_and_unload()

# حفظ النموذج المدمج
merged_model.save_pretrained("merged_model", safe_serialization=True)
tokenizer.save_pretrained("merged_model")
```

---

### الخطوة 7 - دفع النموذج إلى HuggingFace

أخيراً، نقوم بدفع النموذج المحسن إلى HuggingFace:

```python
HF_write = user_secrets.get_secret("HF_WRITE")
login(token=HF_write)

merged_model.push_to_hub("Ruqiya/gemma2_2b_fine_tuned_arabic_dataset", use_auth_token=True)
tokenizer.push_to_hub("Ruqiya/gemma2_2b_fine_tuned_arabic_dataset", use_auth_token=True)
```

---

## الخاتمة
تهانينا! لقد أتممت إعادة ضبط نموذج `Gemma-2b` باستخدام تقنية `qLoRA` على مجموعة البيانات العربية `CIDAR`. أصبح لديك الآن نموذج مخصص يمكنه تنفيذ مهام متعددة تتعلق باللغة العربية. من خلال اتباع هذه الخطوات، تعلمت أيضًا كيفية رفع النموذج على منصة `HuggingFace،` مما يجعله متاحًا للتجربة والاستخدام في المشاريع المستقبلية.

يرجى ملاحظة أن هذا الدرس التعليمي يهدف إلى فهم الخطوات الأساسية، ولكن لتحقيق أداء مرضٍ، قد تحتاج إلى خطوات تحسين إضافية.

لمزيد من الاستكشاف، يمكنك الاطلاع على الروابط أدناه.



## روابط
- [Notebook on Kaggle](https://www.kaggle.com/code/ruqiyas/arabic-finetune-gemma2-2b-using-transformers-qlora/)
- [Finetuned Model on Huggingface](https://huggingface.co/Ruqiya/gemma2_2b_fine_tuned_arabic_dataset)

